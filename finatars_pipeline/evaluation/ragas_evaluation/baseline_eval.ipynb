{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_entity_recall\n",
    ")\n",
    "from ragas import evaluate\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'YOUR_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BaselineEvaluation(evaluation_dataset):\n",
    "    ### Evaluation\n",
    "    print('Running Evaluation...')\n",
    "    # Convert to Ragas format\n",
    "    evaluation_dataset = evaluation_dataset[['question', 'answer', 'contexts', 'ground_truth']] \n",
    "    evaluation_dataset['contexts'] = evaluation_dataset['contexts'].apply(lambda x: np.array(eval(x)))\n",
    "    evaluation_dataset = Dataset.from_pandas(evaluation_dataset)\n",
    "    result = evaluate(\n",
    "        evaluation_dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "            context_entity_recall,\n",
    "        ],\n",
    "    )\n",
    "    print('Evaluation done!')\n",
    "    print(f'Here are the results:\\n{result}')\n",
    "    return result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = pd.read_csv('final_evaluation_datasets/evaluation_dataset_multi_vector.csv')\n",
    "evaluation_dataset.dropna(subset=['answer'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 164 entries, 0 to 179\n",
      "Data columns (total 7 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   question                164 non-null    object\n",
      " 1   contexts                164 non-null    object\n",
      " 2   ground_truth            164 non-null    object\n",
      " 3   ground_truth_chunk_ids  164 non-null    object\n",
      " 4   answer                  164 non-null    object\n",
      " 5   answer_chunk_ids        164 non-null    object\n",
      " 6   type                    164 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 10.2+ KB\n"
     ]
    }
   ],
   "source": [
    "evaluation_dataset.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "evaluation_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/4zp0zzb50w71fn6jwfwny__c0000gn/T/ipykernel_92545/2811769864.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  evaluation_dataset['contexts'] = evaluation_dataset['contexts'].apply(lambda x: np.array(eval(x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 250/250 [03:22<00:00,  1.23it/s]\n",
      "/var/folders/yz/4zp0zzb50w71fn6jwfwny__c0000gn/T/ipykernel_92545/2811769864.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  evaluation_dataset['contexts'] = evaluation_dataset['contexts'].apply(lambda x: np.array(eval(x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation done!\n",
      "Here are the results:\n",
      "{'context_precision': 0.9417, 'faithfulness': 0.7881, 'answer_relevancy': 0.8659, 'context_recall': 1.0000, 'context_entity_recall': 0.2736}\n",
      "Running Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 250/250 [04:11<00:00,  1.01s/it]\n",
      "/var/folders/yz/4zp0zzb50w71fn6jwfwny__c0000gn/T/ipykernel_92545/2811769864.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  evaluation_dataset['contexts'] = evaluation_dataset['contexts'].apply(lambda x: np.array(eval(x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation done!\n",
      "Here are the results:\n",
      "{'context_precision': 0.9000, 'faithfulness': 0.6865, 'answer_relevancy': 0.8694, 'context_recall': 0.9393, 'context_entity_recall': 0.2597}\n",
      "Running Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 250/250 [05:07<00:00,  1.23s/it]\n",
      "/var/folders/yz/4zp0zzb50w71fn6jwfwny__c0000gn/T/ipykernel_92545/2811769864.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  evaluation_dataset['contexts'] = evaluation_dataset['contexts'].apply(lambda x: np.array(eval(x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation done!\n",
      "Here are the results:\n",
      "{'context_precision': 0.9180, 'faithfulness': 0.7069, 'answer_relevancy': 0.8989, 'context_recall': 0.9767, 'context_entity_recall': 0.2816}\n",
      "Running Evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 70/70 [03:37<00:00,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation done!\n",
      "Here are the results:\n",
      "{'context_precision': 0.5042, 'faithfulness': 0.8442, 'answer_relevancy': 0.7380, 'context_recall': 0.8524, 'context_entity_recall': 0.1341}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation (split into 4 chunks due to rate limit)\n",
    "tmp1 = evaluation_dataset.iloc[0:50, :]\n",
    "tmp2 = evaluation_dataset.iloc[50:100, :]\n",
    "tmp3 = evaluation_dataset.iloc[100:150, :]\n",
    "tmp4 = evaluation_dataset.iloc[150:, :]\n",
    "BaselineResult1 = BaselineEvaluation(tmp1) \n",
    "BaselineResult2 = BaselineEvaluation(tmp2) \n",
    "BaselineResult3 = BaselineEvaluation(tmp3) \n",
    "BaselineResult4 = BaselineEvaluation(tmp4) \n",
    "BaselineResult = pd.concat([BaselineResult1, BaselineResult2, BaselineResult3, BaselineResult4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaselineResult.reset_index(inplace=True)\n",
    "BaselineResult.drop(columns=['index'], inplace=True)\n",
    "BaselineResult.to_csv('final_evaluation_datasets/multi_vector_result.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.8844004064293154,\n",
       " 'faithfulness': 0.7355908688982317,\n",
       " 'answer_relevancy': 0.8661014935202744,\n",
       " 'context_recall': 0.961788617886179,\n",
       " 'context_entity_recall': 0.2598870899490058}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results for whole dataset\n",
    "description = BaselineResult.describe()\n",
    "total_results = {\n",
    "    'context_precision': description.loc['mean', 'context_precision'],\n",
    "    'faithfulness': description.loc['mean', 'faithfulness'],\n",
    "    'answer_relevancy': description.loc['mean', 'answer_relevancy'],\n",
    "    'context_recall': description.loc['mean', 'context_recall'],\n",
    "    'context_entity_recall': description.loc['mean', 'context_entity_recall'],\n",
    "}\n",
    "total_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
